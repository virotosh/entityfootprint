{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import itertools,time\n",
    "import sys, os\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from time import time\n",
    "import sys, getopt\n",
    "import Utils as utils\n",
    "from gensim import matutils\n",
    "from collections import namedtuple\n",
    "import json\n",
    "\n",
    "from DataLoader import DataLoader\n",
    "from models import vae\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "\n",
    "    # Number of recommended entities from each view\n",
    "    \"suggestion_count\": 10,\n",
    "    # Number of online snapshots to consider (the latest snapshots)\n",
    "    \"imp_doc_to_consider\": 4,\n",
    "    # True: normalize TF-IDF weights to sum to 1, False: no normalization. TODO: DOES THIS MAKE SENSE?\n",
    "    \"normalize_terms\": True,\n",
    "    # True: use exploration algorithm (Thompson Sampling) for recommendation, False: use the mean of the estimate.\n",
    "    \"Thompson_exploration\": False,\n",
    "    # True: allow the algorithm to show previously recommended items, False: each item can be recommended only once\n",
    "    \"repeated_recommendation\": True,\n",
    "    # A heuristic method to shrink the variance of the posterior (reduce the exploration). it should be in (0,1];\n",
    "    \"exploration_rate\": 1,  # NOT IMPLEMENTED YET\n",
    "    # Number of iterations of the simulated study\n",
    "    \"num_iterations\": 50,\n",
    "    # Number of latent dimensions for data representation\n",
    "    \"num_latent_dims\": 100,\n",
    "    # Number of runs (only for the simulated study, set to 1 for real data setting)\n",
    "    \"num_runs\": 1,  # NOT IMPLEMENTED YET\n",
    "    # True: prepare the data for FOCUS UI but have the interaction in the terminal\n",
    "    \"FOCUS_UI_simulator\": True,\n",
    "    # The directory of the corpus (It should have /corpus.mm, /dictionary.dict, and views_ind_1.npy files)\n",
    "    # \"corpus_directory\": 'corpus1_2/corpus7_sim',\n",
    "    \"corpus_directory\": 'corpus1_2/P01',\n",
    "    # The directory of the new snapshots that will be checked at the beginning of each iteration\n",
    "    \"snapshots_directory\": 'user activity',\n",
    "    # True: Use the simulated user data to simulate the user feedback\n",
    "    \"Simulated_user\": False,\n",
    "    \"save_directory\": 'models_LDA/P01/',\n",
    "    \"sequence_length\": 10,\n",
    "    \"n_step\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"loss_LSTM\": 'mean_squared_error',\n",
    "    \"metrics_LSTM\": ['accuracy'],\n",
    "    \"num_topics\": 20\n",
    "}\n",
    "\n",
    "data_dir = params[\"corpus_directory\"]\n",
    "save_dir = params[\"save_directory\"]\n",
    "data = DataLoader(data_dir, save_dir)\n",
    "data.print_info()\n",
    "\n",
    "n_topics = params[\"num_topics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranform corpus to data frame needed for VAE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df = matutils.corpus2dense(data.corpus, num_terms=data.num_features).T\n",
    "# Minmax normalize\n",
    "scaler_x = MinMaxScaler(feature_range =(0, 1))\n",
    "x = scaler_x.fit_transform(df)\n",
    "data_tr = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary for the corpus\n",
    "vocab = {}\n",
    "for item in data.dictionary.items():\n",
    "    vocab[item[1]] = item[0]\n",
    "print 'Dim Training Data',data_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----------------------------'''\n",
    "\n",
    "'''--------------Global Params---------------'''\n",
    "n_samples_tr = data_tr.shape[0]\n",
    "#n_samples_te = data_te.shape[0]\n",
    "docs_tr = data_tr\n",
    "#docs_te = data_te\n",
    "batch_size=100\n",
    "learning_rate=0.001\n",
    "network_architecture = \\\n",
    "    dict(n_hidden_recog_1=500, # 1st layer encoder neurons\n",
    "         n_hidden_recog_2=500, # 2nd layer encoder neurons\n",
    "         n_hidden_gener_1=500, # 1st layer decoder neurons\n",
    "         n_hidden_gener_2=500, # 2nd layer decoder neurons\n",
    "         n_input=data_tr.shape[1], # MNIST data input (img shape: 28*28)\n",
    "         n_z=n_topics)  # dimensionality of latent space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''-----------------------------'''\n",
    "\n",
    "'''--------------Netowrk Architecture and settings---------------'''\n",
    "\n",
    "def make_network(layer1=500,layer2=500,num_topics=n_topics,bs=100,eta=0.001):\n",
    "    tf.reset_default_graph()\n",
    "    network_architecture = \\\n",
    "        dict(n_hidden_recog_1=layer1, # 1st layer encoder neurons\n",
    "             n_hidden_recog_2=layer2, # 2nd layer encoder neurons\n",
    "             n_hidden_gener_1=500, # 1st layer decoder neurons\n",
    "             n_hidden_gener_2=500, # 2nd layer decoder neurons\n",
    "             n_input=data_tr.shape[1], # MNIST data input (img shape: 28*28)\n",
    "             n_z=num_topics)  # dimensionality of latent space\n",
    "    batch_size=bs\n",
    "    learning_rate=eta\n",
    "    return network_architecture,batch_size,learning_rate\n",
    "\n",
    "\n",
    "\n",
    "'''--------------Methods--------------'''\n",
    "def create_minibatch(data):\n",
    "    rng = np.random.RandomState(10)\n",
    "\n",
    "    while True:\n",
    "        # Return random data samples of a size 'minibatch_size' at each iteration\n",
    "        ixs = rng.randint(data.shape[0], size=batch_size)\n",
    "        yield data[ixs]\n",
    "\n",
    "\n",
    "#def train(network_architecture, learning_rate=0.001,\n",
    "def train(network_architecture, minibatches, learning_rate=0.001,\n",
    "          batch_size=100, training_epochs=10, display_step=5):\n",
    "    \n",
    "    _vae = vae.VariationalAutoencoder(network_architecture, \n",
    "                                 learning_rate=learning_rate, \n",
    "                                 batch_size=batch_size)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(n_samples_tr / batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            #batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "            batch_xs = minibatches.next()\n",
    "\n",
    "            # Fit training using batch data\n",
    "            cost = _vae.partial_fit(batch_xs)\n",
    "            # Compute average loss\n",
    "            avg_cost += cost / n_samples_tr * batch_size\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \n",
    "                  \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    return _vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = int(500)\n",
    "s = int(500)\n",
    "t = int(n_topics)\n",
    "b = int(100)\n",
    "r = float(0.001)\n",
    "e = int(75)\n",
    "minibatches = create_minibatch(docs_tr.astype('float32'))\n",
    "network_architecture,batch_size,learning_rate=make_network(f,s,t,b,r)\n",
    "print network_architecture\n",
    "print batch_size\n",
    "print n_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_latent = train(network_architecture, minibatches, training_epochs=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = params[\"sequence_length\"]\n",
    "STEP = params[\"n_step\"]\n",
    "BATCH_SIZE = params[\"batch_size\"]\n",
    "\n",
    "TaggedDocument = namedtuple('TaggedDocument', 'tags words')\n",
    "all_docs = []\n",
    "tokenized_texts = []\n",
    "doc_id = 0\n",
    "len_doc = []\n",
    "all_entities = []\n",
    "docs_topics = np.zeros((len(data.corpus), n_topics))\n",
    "for i, doc in enumerate(data.corpus):\n",
    "    words = doc\n",
    "    words = [data.dictionary[w[0]] for w in words]\n",
    "    all_docs.append(TaggedDocument([doc_id], words))\n",
    "    tokenized_texts.append((words))\n",
    "    doc_id += 1\n",
    "    len_doc.append(len(words))\n",
    "    all_entities = all_entities + words\n",
    "\n",
    "    topics_i = latent_prop[i]\n",
    "    for j in range(len(topics_i)):\n",
    "        docs_topics[i, j] = topics_i[j]\n",
    "\n",
    "M_T_sparse = matutils.corpus2csc(data.corpus, num_terms=data.num_features,\n",
    "                                 num_docs=data.num_data, num_nnz=data.corpus.num_nnz)\n",
    "\n",
    "screens = []\n",
    "next_screens = []\n",
    "M_T_screens = []\n",
    "M_T_next_screens = []\n",
    "for i in range(0, len(docs_topics) - SEQUENCE_LEN):\n",
    "    screens.append(docs_topics[i: i + SEQUENCE_LEN])\n",
    "    next_screens.append(docs_topics[i + SEQUENCE_LEN])\n",
    "    M_T_screens.append(M_T_sparse[:, i: i + SEQUENCE_LEN])\n",
    "    M_T_next_screens.append(M_T_sparse[:, i + SEQUENCE_LEN])\n",
    "print('nb sequences:', len(screens))\n",
    "print screens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No shuffling train and test set\n",
    "print len(screens)\n",
    "print '---'\n",
    "print next_screens[0]\n",
    "cut_index = int((len(screens)-10) * (1.-(20/100.)))\n",
    "print cut_index\n",
    "screens_train, screens_test = screens[:cut_index], screens[cut_index:]\n",
    "next_screens_train, next_screens_test = next_screens[:cut_index], next_screens[cut_index:]\n",
    "\n",
    "# Print the train set\n",
    "print screens_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = np.arange(start=0, stop=len(screens), step=1)\n",
    "index_train, index_test = test_seq[:cut_index], test_seq[cut_index:]\n",
    "M_T_screens_test = [M_T_screens[i] for i in list(index_test)]\n",
    "M_T_next_screens_test = [M_T_next_screens[i] for i in list(index_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding\n",
    "\n",
    "# screen_list, next_screen_list, batch_size = screens, next_screen, BATCH_SIZE\n",
    "# Data generator for fit and evaluate\n",
    "def generator(screen_list, next_screen_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN, n_topics), dtype=np.float64)\n",
    "        y = np.zeros((batch_size, n_topics), dtype=np.float64)\n",
    "        for i in range(batch_size):\n",
    "            for t, tpc in enumerate(screen_list[index % len(screen_list)]):\n",
    "                x[i, t, :] = tpc\n",
    "            y[i, :] = next_screen_list[index % len(screen_list)]\n",
    "            index = index + 1\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "def get_model(dropout=0.5):\n",
    "    \"Constructs an LSTM model and adds different layers to it\"\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    # model.add(Embedding(input_dim=n_topics, output_dim=128))\n",
    "    # model.add(Bidirectional(LSTM(150)))\n",
    "    model.add(Bidirectional(LSTM(50, activation=\"relu\"), input_shape=(SEQUENCE_LEN, n_topics)))  # , activation=\"relu\"\n",
    "    if dropout > 0:\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(n_topics))\n",
    "    # model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "\n",
    "# Functions from keras-team/keras/blob/master/examples/lstm_text_generation.py\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas_topic = np.random.multinomial(1, preds, 1)\n",
    "    sampled_topic = np.flatnonzero(probas_topic)\n",
    "    # lda_topics[sampled_topic[0]]\n",
    "    probas_entity = np.random.multinomial(n_entities, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    examples_file.write('\\n----- Generating topic after Epoch: %d\\n' % epoch)\n",
    "\n",
    "    # Randomly pick a seed sequence\n",
    "    seed_index = np.random.randint(len(screens_train + screens_test))\n",
    "    seed = (screens_train + screens_test)[seed_index]\n",
    "\n",
    "    screen = seed\n",
    "    # examples_file = screen.tolist()\n",
    "    examples_file.write('----- Generating with seed:\\n\"' + ' ' + str(screen) + '\"\\n')\n",
    "    for i in range(10):\n",
    "        x_pred = np.zeros((1, SEQUENCE_LEN, n_topics))\n",
    "        for t, top in enumerate(screen):\n",
    "            x_pred[0, t, :] = top\n",
    "\n",
    "        preds = model.predict_proba(x_pred, verbose=0)[0]\n",
    "\n",
    "        screen = screen[1:]\n",
    "        screen = np.vstack([screen, preds])\n",
    "\n",
    "        # examples_file.append(preds.tolist())\n",
    "        # return examples_file\n",
    "        examples_file.write(\" \" + str(preds))\n",
    "        examples_file.write('\\n')\n",
    "    examples_file.write('=' * 80 + '\\n')\n",
    "    examples_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(screens_train, next_screens_train), (screens_test, next_screens_test), (\n",
    "#index_train, index_test) = utils.shuffle_and_split_training_set_LDA(\n",
    "#    screens, next_screens, SEQUENCE_LEN)\n",
    "\n",
    "#M_T_screens_test = [M_T_screens[i] for i in list(index_test)]\n",
    "#M_T_next_screens_test = [M_T_next_screens[i] for i in list(index_test)]\n",
    "# x = np.zeros((len(screens), SEQUENCE_LEN, n_topics), dtype=np.float64)\n",
    "# y = np.zeros((len(screens), n_topics), dtype=np.float64)\n",
    "# for i, scr in enumerate(screens):\n",
    "#     for t, tpc in enumerate(scr):\n",
    "#         x[i, t, :] = tpc\n",
    "#     y[i, :] = next_screen[i]\n",
    "\n",
    "if not os.path.isdir('./checkpoints/'):\n",
    "    os.makedirs('./checkpoints/')\n",
    "\n",
    "model = get_model()\n",
    "print(model.summary())\n",
    "model.compile(loss='mean_squared_error', optimizer=\"adam\",\n",
    "              metrics=['accuracy'])  # categorical_crossentropy , mean_squared_error, logcosh\n",
    "''' categorical_cross_entropy'''\n",
    "file_path = \"./checkpoints/LSTM_LDA-epoch{epoch:03d}-topics%d-sequence%d-\" \\\n",
    "            \"loss{loss:.4f}-acc{accuracy:.4f}-val_loss{val_loss:.4f}-val_acc{val_accuracy:.4f}\" % \\\n",
    "            (n_topics, SEQUENCE_LEN)\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_accuracy', save_best_only=True)\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n",
    "callbacks_list = [checkpoint, print_callback, early_stopping]\n",
    "\n",
    "examples = 'examples_topic_vector.txt'\n",
    "examples_file = open(examples, \"w\")\n",
    "# examples_file = []\n",
    "history = model.fit_generator(generator(screens_train, next_screens_train, BATCH_SIZE),\n",
    "                              steps_per_epoch=int(len(screens_train) / BATCH_SIZE) + 1,\n",
    "                              epochs=100,\n",
    "                              callbacks=callbacks_list,\n",
    "                              validation_data=generator(screens_test, next_screens_test, BATCH_SIZE),\n",
    "                              validation_steps=int(len(screens_test) / BATCH_SIZE) + 1)\n",
    "\n",
    "# model.save(save_dir + \"/\" + 'LSTM_LDA.final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"**************  Generate Recommendations  **************\"\n",
    "# Future step\n",
    "n_future = 1\n",
    "# Amount test data\n",
    "n_tests = int(len(screens_test)/100)*100\n",
    "precision, recall, precision_kw, recall_kw, \\\n",
    "precision_app, recall_app, precision_ppl, recall_ppl = [], [], [], [], [], [], [], []\n",
    "# Randomly pick a seed sequence\n",
    "#seed_index = np.random.randint(len(screens_test), size=n_tests)\n",
    "# Manual pick a seed sequence\n",
    "seed_index = np.arange(start=0, stop=n_tests, step=1)\n",
    "n_empty_app = 0\n",
    "\n",
    "test_ =  np.zeros(shape=(n_tests,20))\n",
    "test_kw = []\n",
    "test_ppl = []\n",
    "test_app = []\n",
    "test_flat_entities_original = []\n",
    "for k in range(n_tests):\n",
    "    #print(k)\n",
    "    entities_original = []\n",
    "    flat_entities_original = []\n",
    "    recommended_entities = []\n",
    "\n",
    "    # seed = screens_test[seed_index[k]]\n",
    "    seed = index_test[seed_index[k]]\n",
    "    screen = screens[seed]\n",
    "    # entities_screen = M_T_next_screens_test[seed_index[k]:seed_index[k]+n_future]\n",
    "    entities_screen = M_T_next_screens[index_test[seed_index[k]]:index_test[seed_index[k]] + n_future]\n",
    "    for n in range(n_future):\n",
    "        entities_original += [data.dictionary[i] for i in entities_screen[n].indices]\n",
    "    flat_entities_original = list(dict.fromkeys(entities_original))\n",
    "    #for i in range(len(flat_entities_original)):\n",
    "    #    print flat_entities_original[i], data.dictionary.token2id[flat_entities_original[i]], data.views_ind[data.dictionary.token2id[flat_entities_original[i]]]\n",
    "    flat_entities_original_views = [data.views_ind[data.dictionary.token2id[flat_entities_original[i]]] for i in range(len(flat_entities_original))]\n",
    "    original_kw = [flat_entities_original[i] for i in range(len(flat_entities_original)) if flat_entities_original_views[i] == 1]\n",
    "    original_app = [flat_entities_original[i] for i in range(len(flat_entities_original)) if flat_entities_original_views[i] == 2]\n",
    "    original_ppl = [flat_entities_original[i] for i in range(len(flat_entities_original)) if flat_entities_original_views[i] == 3]\n",
    "    #print original_app\n",
    "    test_kw.append(original_kw)\n",
    "    test_app.append(original_app)\n",
    "    test_ppl.append(original_ppl)\n",
    "    test_flat_entities_original.append(flat_entities_original)\n",
    "\n",
    "    \"=====================================    VAE + LSTM   ============================\"\n",
    "\n",
    "    \"Prepare the input for the model\"\n",
    "    x_pred = np.zeros((1, SEQUENCE_LEN, n_topics))\n",
    "    for t, top in enumerate(screen):\n",
    "        x_pred[0, t, :] = top\n",
    "    \"predict the next latent vector (or the topic vector)\"\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    #print preds\n",
    "    test_[k] = preds\n",
    "#print test_\n",
    "transformed_test_ = []\n",
    "test_after = np.array_split(test_, n_tests/100)\n",
    "for b in test_after:\n",
    "    for v in vae_latent.generate(b):\n",
    "        transformed_test_.append(v)\n",
    "\n",
    "#transformed_test_ = vae_latent.generate(test_)\n",
    "for i in range(n_tests):\n",
    "    recommended_entities = []\n",
    "    \n",
    "    original_kw = test_kw[i]\n",
    "    original_app = test_app[i]\n",
    "    original_ppl = test_ppl[i]\n",
    "    flat_entities_original = test_flat_entities_original[i]\n",
    "    \n",
    "    print original_app\n",
    "    \n",
    "    if len(original_app) == 0:\n",
    "        n_empty_app+=1\n",
    "    predictionList = []\n",
    "    #print np.around(transformed_test_[i].astype(float), decimals=3)\n",
    "    #_feature_names = transformed_test_[i].argsort()[::-1][:10000]\n",
    "    _feature_names = transformed_test_[i]\n",
    "    for ind, v in enumerate(_feature_names):\n",
    "        views_ind_ij = data.views_ind[ind]\n",
    "        probab_i = v\n",
    "        predictionList.append((ind, probab_i, views_ind_ij))\n",
    "        #if data.views_ind[item] == 2:\n",
    "        #    print i, data.dictionary[item], data.views_ind[item]\n",
    "\n",
    "    def takeSecond(elem):\n",
    "        return elem[1]\n",
    "\n",
    "    \n",
    "    #print predictionList[:10]\n",
    "    sorted_terms = sorted(predictionList, key=takeSecond, reverse=True)\n",
    "    #print sorted_terms[:10]\n",
    "\n",
    "    def flatRecom(seq):\n",
    "        seen = set()\n",
    "        seen_add = seen.add\n",
    "        return [x for x in seq if not (x[0] in seen or seen_add(x[0]))]\n",
    "\n",
    "\n",
    "    flat_entities_recommended = flatRecom(sorted_terms)\n",
    "    sorted_views_list = []  # sorted ranked list of each view\n",
    "    for view in range(1, data.num_views):\n",
    "        sorted_view = [term[0] for term in flat_entities_recommended if term[2] == view]\n",
    "        sorted_views_list.append(sorted_view)\n",
    "    #print sorted_views_list\n",
    "    # IN TERMINAL USER INTERFACE\n",
    "    if params[\"FOCUS_UI_simulator\"]:\n",
    "\n",
    "        for view in range(1, data.num_views):\n",
    "            print('view %d:' % view)\n",
    "            for i in range(min(params[\"suggestion_count\"], data.num_items_per_view[view])):\n",
    "                print('    %d,' % sorted_views_list[view - 1][i] + ' ' + data.feature_names[\n",
    "                    sorted_views_list[view - 1][i]])\n",
    "\n",
    "        # organize the recommentations in the right format\n",
    "        data_output = {}\n",
    "        data_output[\"keywords\"] = [(sorted_views_list[0][i], data.feature_names[sorted_views_list[0][i]])\n",
    "                                   for i in range(min(params[\"suggestion_count\"], data.num_items_per_view[1]))]\n",
    "        data_output[\"applications\"] = [(sorted_views_list[1][i], data.feature_names[sorted_views_list[1][i]])\n",
    "                                       for i in range(min(params[\"suggestion_count\"], data.num_items_per_view[2]))]\n",
    "        data_output[\"people\"] = [(sorted_views_list[2][i], data.feature_names[sorted_views_list[2][i]])\n",
    "                                 for i in range(min(params[\"suggestion_count\"], data.num_items_per_view[3]))]\n",
    "\n",
    "        # # for now write everything in a file\n",
    "        # with open('data.txt', 'w') as outfile:\n",
    "        #     json.dump(data_output, outfile)\n",
    "\n",
    "    recommended_kw = [data.feature_names[sorted_views_list[0][i]] for i in range(len(data_output[\"keywords\"]))]\n",
    "    recommended_app = [data.feature_names[sorted_views_list[1][i]] for i in range(len(data_output[\"applications\"]))]\n",
    "    recommended_ppl = [data.feature_names[sorted_views_list[2][i]] for i in range(len(data_output[\"people\"]))]\n",
    "\n",
    "    recommended_entities = recommended_kw + recommended_app + recommended_ppl\n",
    "    # def check_element(a, b):\n",
    "    #   return not set(a).isdisjoint(b)\n",
    "    #\n",
    "    # check_element(recommended_kw, flat_entities_original)\n",
    "    # check_element(recommended_app, flat_entities_original)\n",
    "    # check_element(recommended_ppl, flat_entities_original)\n",
    "\n",
    "    c = []\n",
    "    for bx in recommended_entities:\n",
    "        if bx in flat_entities_original:\n",
    "            c.append(bx)\n",
    "    if c:\n",
    "        print('these are the elements of list a that are present in list b:')\n",
    "        print(c)\n",
    "    else:\n",
    "        print('no elements of list a are in list b')\n",
    "    print(float(len(c)) / float(len(recommended_entities)), ' correct recommendations in ', n_future, 'future pages')\n",
    "\n",
    "    precision.append(float(len(c)) / float(len(recommended_entities)))\n",
    "    recall.append(float(len(c)) / float(len(flat_entities_original) + 1e-10))\n",
    "\n",
    "    c_kw = []\n",
    "    for bx_kw in recommended_kw:\n",
    "        if bx_kw in original_kw:\n",
    "            c_kw.append(bx_kw)\n",
    "\n",
    "    precision_kw.append(float(len(c_kw)) / float(len(recommended_kw)))\n",
    "    if (original_kw != []):\n",
    "        recall_kw.append(float(len(c_kw)) / float(len(original_kw)))\n",
    "    else:\n",
    "        recall_kw.append(None)\n",
    "\n",
    "    c_app = []\n",
    "    for bx_app in recommended_app:\n",
    "        if bx_app in original_app:\n",
    "            c_app.append(bx_app)\n",
    "\n",
    "    precision_app.append(float(len(c_app)) / float(len(recommended_app)))\n",
    "    if (original_app != []):\n",
    "        recall_app.append(float(len(c_app)) / float(len(original_app)))\n",
    "    else:\n",
    "        recall_app.append(None)\n",
    "\n",
    "    c_ppl = []\n",
    "    for bx_ppl in recommended_ppl:\n",
    "        if bx_ppl in original_ppl:\n",
    "            c_ppl.append(bx_ppl)\n",
    "\n",
    "    precision_ppl.append(float(len(c_ppl)) / float(len(recommended_ppl)))\n",
    "    if(original_ppl != []):\n",
    "        recall_ppl.append(float(len(c_ppl)) / float(len(original_ppl)))\n",
    "    else:\n",
    "        recall_ppl.append(None)\n",
    "\n",
    "precision_avg = np.mean(precision)\n",
    "precision_kw_avg = np.mean(precision_kw)\n",
    "precision_ppl_avg = np.mean(precision_ppl)\n",
    "precision_app_avg = np.mean(precision_app)\n",
    "\n",
    "precision_std = np.std(precision)\n",
    "precision_kw_std = np.std(precision_kw)\n",
    "precision_ppl_std = np.std(precision_ppl)\n",
    "precision_app_std = np.std(precision_app)\n",
    "\n",
    "recall_avg = np.mean([l for l in (recall) if l != None])\n",
    "recall_kw_avg = np.mean([l for l in (recall_kw) if l != None])\n",
    "recall_ppl_avg = np.mean([l for l in (recall_ppl) if l != None])\n",
    "recall_app_avg = np.mean([l for l in (recall_app) if l != None])\n",
    "\n",
    "recall_std = np.std([l for l in (recall) if l != None])\n",
    "recall_kw_std = np.std([l for l in (recall_kw) if l != None])\n",
    "recall_ppl_std = np.std([l for l in (recall_ppl) if l != None])\n",
    "recall_app_std = np.std([l for l in (recall_app) if l != None])\n",
    "\n",
    "print('precision: ', precision_avg, '+', precision_std)\n",
    "print('precision_kw: ', precision_kw_avg, '+', precision_kw_std)\n",
    "print('precision_ppl: ', precision_ppl_avg, '+', precision_ppl_std)\n",
    "print('precision_app: ', precision_app_avg, '+', precision_app_std)\n",
    "\n",
    "print('recall: ', recall_avg, '+', recall_std)\n",
    "print('recall_kw: ', recall_kw_avg, '+', recall_kw_std)\n",
    "print('recall_ppl: ', recall_ppl_avg, '+', recall_ppl_std)\n",
    "print('recall_app: ', recall_app_avg, '+', recall_app_std)\n",
    "print n_empty_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
